{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec202ada",
   "metadata": {},
   "source": [
    "# Assignment2 - Supervised Learning flow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06a2810",
   "metadata": {},
   "source": [
    "# Part 1(a) Student details:\n",
    "* Please write the First-Name, First letter of Last-Name and last 4 digits of the i.d. for each student. "
   ]
  },
  {
   "cell_type": "code",
   "id": "ca16486b",
   "metadata": {},
   "source": [
    "# stundent details:\n",
    "# Michael O J 5140\n",
    "# Agam M 6895\n",
    "# Ofek L 8567"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "abb64166-b32e-4b86-a7ee-84106c8b3a92",
   "metadata": {},
   "source": [
    "## Part 1(b) - Chat-GPT/other AI-agent/other assistance used:\n",
    "* If you changed the prompt until you got a satisfying answer, please add all versions\n",
    "* don't delete \"pre\" tags, so new-line is supported\n",
    "* double click the following markdown cell to change\n",
    "* press shift+enter to view\n",
    "* Add information:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea93f0d3-7aa1-4a95-b084-395f5f7c6c0c",
   "metadata": {},
   "source": [
    "    #### Add information in this Markdown cell (double click to change, shift-enter to view)\n",
    "<pre>   \n",
    "AI agent name: Coplilot-{Gpt, Claude, etc...}\n",
    "Goal: Help us write proper code, prevent errors and to understand the pipeline better.\n",
    "Propmpt1:\n",
    "    \"Fix the highlighted code\"\n",
    "Propmpt2:\n",
    "    \"Why do i get this error?\"\n",
    "Propmpt3:\n",
    "    \"Explain hyperparameters and their importance in ML\"\n",
    "\n",
    "\n",
    "AI agent name 2:\n",
    "Goal:\n",
    "Propmpt1:\n",
    "\n",
    "Propmpt2:\n",
    "\n",
    "Propmpt3:\n",
    "\n",
    "\n",
    "Other assistanse:\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fbaab-e5a5-40ed-be0f-9a2cb7931a4b",
   "metadata": {},
   "source": [
    "    ## Part 1(c) - Learning Problem and dataset explaination.\n",
    "* Please explain in one paragraph\n",
    "* don't delete \"pre\" tags, so new-line is supported\n",
    "* double click the following markdown cell to change\n",
    "* press shift+enter to view\n",
    "* Add explaining text:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd8565df-2778-4b89-83bd-6b227063789f",
   "metadata": {},
   "source": [
    "    #### Add information in this Markdown cell (double click to change, shift-enter to view)\n",
    "<pre>\n",
    "The learning problem involves predicting the median house value (`MedHouseVal`) based on various features such as the average number of rooms (`AveRooms`), population size (`Population`), and other housing-related attributes. The dataset used for this task is a housing dataset split into training (`housing_train.csv`) and test (`housing_test.csv`) sets. The training set is used to build and optimize machine learning models, while the test set is used to evaluate the model's performance. The goal is to identify the best combination of feature engineering techniques and algorithms to achieve accurate predictions of house prices.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df67c2ee-87c8-499c-a04f-1853c332f51d",
   "metadata": {},
   "source": [
    "## Part 2 - Initial Preparations \n",
    "You could add as many code cells as needed"
   ]
  },
  {
   "cell_type": "code",
   "id": "b29b3454-b568-4614-8017-f15b3c59fc55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-30T13:57:12.787532Z",
     "start_time": "2025-05-30T13:57:11.920839Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from sklearn.pipeline import Pipeline"
   ],
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[14], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnumpy\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mtime\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mplt\u001B[39;00m\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\__init__.py:151\u001B[0m\n\u001B[0;32m    132\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomputation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;28meval\u001B[39m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreshape\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m    135\u001B[0m     concat,\n\u001B[0;32m    136\u001B[0m     lreshape,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    148\u001B[0m     qcut,\n\u001B[0;32m    149\u001B[0m )\n\u001B[1;32m--> 151\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m api, arrays, errors, io, plotting, tseries\n\u001B[0;32m    152\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m testing\n\u001B[0;32m    153\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_print_versions\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m show_versions\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\api\\__init__.py:2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\" public toolkit API \"\"\"\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mapi\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      3\u001B[0m     extensions,\n\u001B[0;32m      4\u001B[0m     indexers,\n\u001B[0;32m      5\u001B[0m     interchange,\n\u001B[0;32m      6\u001B[0m     types,\n\u001B[0;32m      7\u001B[0m     typing,\n\u001B[0;32m      8\u001B[0m )\n\u001B[0;32m     10\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minterchange\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     12\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mextensions\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtyping\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     16\u001B[0m ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\api\\typing\\__init__.py:31\u001B[0m\n\u001B[0;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mwindow\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     20\u001B[0m     Expanding,\n\u001B[0;32m     21\u001B[0m     ExpandingGroupby,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     26\u001B[0m     Window,\n\u001B[0;32m     27\u001B[0m )\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# TODO: Can't import Styler without importing jinja2\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# from pandas.io.formats.style import Styler\u001B[39;00m\n\u001B[1;32m---> 31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_json\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m JsonReader\n\u001B[0;32m     32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstata\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m StataReader\n\u001B[0;32m     34\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     35\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDataFrameGroupBy\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mDatetimeIndexResamplerGroupby\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     54\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWindow\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     55\u001B[0m ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\io\\json\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_json\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     read_json,\n\u001B[0;32m      3\u001B[0m     to_json,\n\u001B[0;32m      4\u001B[0m     ujson_dumps,\n\u001B[0;32m      5\u001B[0m     ujson_loads,\n\u001B[0;32m      6\u001B[0m )\n\u001B[0;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_table_schema\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m build_table_schema\n\u001B[0;32m      9\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mujson_dumps\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     11\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mujson_loads\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbuild_table_schema\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     15\u001B[0m ]\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\io\\json\\_json.py:71\u001B[0m\n\u001B[0;32m     66\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_normalize\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m convert_to_line_delimits\n\u001B[0;32m     67\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mjson\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_table_schema\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     68\u001B[0m     build_table_schema,\n\u001B[0;32m     69\u001B[0m     parse_table_schema,\n\u001B[0;32m     70\u001B[0m )\n\u001B[1;32m---> 71\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m validate_integer\n\u001B[0;32m     73\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[0;32m     74\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mcollections\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabc\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     75\u001B[0m         Hashable,\n\u001B[0;32m     76\u001B[0m         Mapping,\n\u001B[0;32m     77\u001B[0m     )\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\__init__.py:1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mio\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mreaders\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m      2\u001B[0m     TextFileReader,\n\u001B[0;32m      3\u001B[0m     TextParser,\n\u001B[0;32m      4\u001B[0m     read_csv,\n\u001B[0;32m      5\u001B[0m     read_fwf,\n\u001B[0;32m      6\u001B[0m     read_table,\n\u001B[0;32m      7\u001B[0m )\n\u001B[0;32m      9\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextFileReader\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTextParser\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_csv\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_fwf\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mread_table\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[1;32m~\\PycharmProjects\\local_playground_project\\.venv\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:32\u001B[0m\n\u001B[0;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_config\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m using_copy_on_write\n\u001B[0;32m     31\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_libs\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m lib\n\u001B[1;32m---> 32\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_libs\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mparsers\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m STR_NA_VALUES\n\u001B[0;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merrors\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[0;32m     34\u001B[0m     AbstractMethodError,\n\u001B[0;32m     35\u001B[0m     ParserWarning,\n\u001B[0;32m     36\u001B[0m )\n\u001B[0;32m     37\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21;01mpandas\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_decorators\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m Appender\n",
      "File \u001B[1;32mparsers.pyx:1418\u001B[0m, in \u001B[0;36minit pandas._libs.parsers\u001B[1;34m()\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: partially initialized module 'pandas' has no attribute '_pandas_parser_CAPI' (most likely due to a circular import)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "4b47fe56-d611-4b28-be92-673db4d56400",
   "metadata": {},
   "source": [
    "df_train = pd.read_csv(\"housing_train.csv\")\n",
    "df_test = pd.read_csv(\"housing_test.csv\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "53e27610-b640-4db0-80c9-789b5f5fae58",
   "metadata": {},
   "source": "df_train.head()",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df_test.head()",
   "id": "66b1acdd33498f7d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# data cleansing\n",
    "def clean_data(df):\n",
    "    df.dropna(inplace=True)\n",
    "    df.drop_duplicates(inplace=True)\n",
    "    return df\n",
    "\n",
    "def remove_outliers(df, column, threshold=3):\n",
    "    mean = df[column].mean()\n",
    "    std_dev = df[column].std()\n",
    "    lower_bound = mean - threshold * std_dev\n",
    "    upper_bound = mean + threshold * std_dev\n",
    "    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n"
   ],
   "id": "545bb43227862566"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(f\"Train set number of nulls: {df_train.isnull().sum()}\")\n",
    "df_train = clean_data(df_train)\n",
    "df_train = remove_outliers(df_train, 'MedHouseVal')\n",
    "df_train = remove_outliers(df_train, 'AveRooms')\n",
    "df_train = remove_outliers(df_train, 'Population')\n",
    "print(f\"Train set number of nulls after cleaning: {df_train.isnull().sum()}\")"
   ],
   "id": "83d4c1f64ad8db91",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "figure, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "col1, col2, col3 = \"AveRooms\", \"Population\", \"MedHouseVal\"\n",
    "\n",
    "axes[0].scatter(df_train[col1], df_train[col2])\n",
    "axes[0].set_xlabel(col1)\n",
    "axes[0].set_ylabel(col2)\n",
    "axes[0].set_title(f'{col1} vs {col2}')\n",
    "\n",
    "axes[1].scatter(df_train[col1], df_train[col3])\n",
    "axes[1].set_xlabel(col1)\n",
    "axes[1].set_ylabel(col3)\n",
    "axes[1].set_title(f'{col1} vs {col3}')\n",
    "\n",
    "axes[2].scatter(df_train[col2], df_train[col3])\n",
    "axes[2].set_xlabel(col2)\n",
    "axes[2].set_ylabel(col3)\n",
    "axes[2].set_title(f'{col2} vs {col3}')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "8ffb8d6e77a9933",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "figure , axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].hist(df_train[col1], bins=20, color='red', alpha=0.7)\n",
    "axes[0].set_title(f'{col1} Histogram')\n",
    "\n",
    "axes[1].hist(df_train[col2], bins=20, color='green', alpha=0.7)\n",
    "axes[1].set_title(f'{col2} Histogram')\n",
    "\n",
    "axes[2].hist(df_train[col3], bins=20, color='blue', alpha=0.7)\n",
    "axes[2].set_title(f'{col3} Histogram')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "1ff1cdb1f5ee8e39",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "corr = df_train.corr(method = 'pearson')\n",
    "corr"
   ],
   "id": "7ab2872f11480471",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The purpose of the visualizations is to explore the relationships and distributions of key features in the dataset. Scatter plots are used to examine potential correlations between variables (such as average rooms, population, and median house value), while histograms provide insights into the distribution of each feature, helping to identify patterns, outliers, or skewness in the data.",
   "id": "dbb5a1e77cf11eba"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 3 - Experiments\n",
    "You could add as many code cells as needed"
   ],
   "id": "832ff4e246efb172"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# Feature engineering options\n",
    "feature_options = [\n",
    "    [],  # No transformation\n",
    "    ['scale'],  # Standard scaling\n",
    "    ['poly2'],  # Polynomial degree 2\n",
    "    ['selectk'],  # Feature selection\n",
    "    ['scale', 'poly2'],  # Scaling + Polynomial\n",
    "    ['scale', 'selectk'],  # Scaling + Feature selection\n",
    "    ['poly2', 'selectk'],  # Polynomial + Feature selection\n",
    "    ['scale', 'poly2', 'selectk']  # All transformations\n",
    "]\n",
    "\n",
    "# Models and their hyperparameters\n",
    "models = {\n",
    "    'LinearRegression': {\n",
    "        'model': LinearRegression(),\n",
    "        'params': {}\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5, None]}\n",
    "    },\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {'model__alpha': [0.1, 1, 10]}\n",
    "    },\n",
    "    'NeuralNetwork': {\n",
    "        'model': MLPRegressor(random_state=42, max_iter=500),\n",
    "        'params': {'model__hidden_layer_sizes': [(50,), (100,)], 'model__activation': ['relu', 'tanh']}\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'model': XGBRegressor(tree_method='hist', device='cuda', random_state=42),\n",
    "        'params': {'model__n_estimators': [50, 100], 'model__max_depth': [3, 5, None]}\n",
    "    }\n",
    "}\n",
    "results = []\n",
    "\n",
    "X = df_train.drop('MedHouseVal', axis=1)\n",
    "y = df_train['MedHouseVal']"
   ],
   "id": "fb3e6ea8698ff028",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for feats in feature_options:\n",
    "    start_time = time.time()\n",
    "    steps = []\n",
    "    feats_name = []\n",
    "    if 'scale' in feats:\n",
    "        steps.append(('scaler', StandardScaler()))\n",
    "        feats_name.append('scale')\n",
    "    if 'poly2' in feats:\n",
    "        steps.append(('poly', PolynomialFeatures(degree=2, include_bias=False)))\n",
    "        feats_name.append('poly2')\n",
    "    if 'selectk' in feats:\n",
    "        steps.append(('selectk', SelectKBest(score_func=f_regression, k=2)))\n",
    "        feats_name.append('selectk')\n",
    "    for model_name, model_dict in models.items():\n",
    "        print(f\"Running model: {model_name} with features: {feats_name}\")\n",
    "\n",
    "        pipe_steps = steps + [('model', model_dict['model'])]\n",
    "        pipe = Pipeline(pipe_steps)\n",
    "        param_grid = model_dict['params']\n",
    "\n",
    "        grid = GridSearchCV(pipe, param_grid, cv=5, scoring=make_scorer(mean_squared_error, greater_is_better=False), n_jobs=-1)\n",
    "        grid.fit(X, y)\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        results.append({\n",
    "            'Feature Engineering': '+'.join(feats_name) if feats_name else 'none',\n",
    "            'Model': model_name,\n",
    "            'Best Params': grid.best_params_,\n",
    "            'CV Mean MSE': -grid.best_score_,\n",
    "            'R² Score':  grid.best_estimator_.score(X, y),\n",
    "            'Elapsed Time (s)': elapsed_time\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df = results_df.sort_values('R² Score', ascending=False)\n",
    "display(results_df)"
   ],
   "id": "9ff89902e83f1614",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Part 4 - Training \n",
    "Use the best combination of feature engineering, model (algorithm and hyperparameters) from the experiment part (part 3)"
   ],
   "id": "8c45f578b62bf1c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "best_result = results_df.iloc[0]\n",
    "best_features = best_result['Feature Engineering'].split('+') if best_result['Feature Engineering'] != 'none' else []\n",
    "best_model_name = best_result['Model']\n",
    "best_params = {k: v for k, v in best_result['Best Params'].items() if k != 'model__model'}\n",
    "\n",
    "steps = []\n",
    "if 'scale' in best_features:\n",
    "    steps.append(('scaler', StandardScaler()))\n",
    "if 'poly2' in best_features:\n",
    "    steps.append(('poly', PolynomialFeatures(degree=2, include_bias=False)))\n",
    "if 'selectk' in best_features:\n",
    "    steps.append(('selectk', SelectKBest(score_func=f_regression, k=2)))\n",
    "\n",
    "best_model = models[best_model_name]['model']\n",
    "steps.append(('model', best_model))\n",
    "\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "pipeline.set_params(**best_params)\n",
    "\n",
    "pipeline.fit(X, y)"
   ],
   "id": "f90d70dcb416332d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Part 5 - Apply on test and show model performance estimation",
   "id": "4e5f796403eda30"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X_test = df_test.drop('MedHouseVal', axis=1)\n",
    "y_test = df_test['MedHouseVal']\n",
    "\n",
    "y_pred = pipeline.predict(X_test)\n",
    "\n",
    "cv_scores = cross_val_score(pipeline, X_test, y_test, cv=5, scoring=make_scorer(mean_squared_error, greater_is_better=False))\n",
    "mean_cv_mse = -cv_scores.mean()\n",
    "score = pipeline.score(X_test, y_test)\n",
    "\n",
    "print(f\"Cross-Validation Mean MSE on Test Set: {mean_cv_mse}\")\n",
    "print(f\"R² Score on Test Set: {score}\")\n",
    "print(f\"Model Accuracy: {score*100:.2f}%\")\n",
    "\n",
    "pd.DataFrame({'Actual': y_test, 'Predicted': y_pred}).head()"
   ],
   "id": "d43183857b054346",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    print(\"Feature Importance:\")\n",
    "    display(importance_df)\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Feature Importance')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.show()\n",
    "\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(y_test, residuals, alpha=0.7, color='purple')\n",
    "plt.axhline(y=0, color='red', linestyle='--')\n",
    "plt.xlabel('Actual Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Analysis')\n",
    "plt.show()"
   ],
   "id": "a793ff2aa51876c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
